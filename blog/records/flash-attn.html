<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" integrity="sha256-dABdfBfUoC8vJUBOwGVdm8L9qlMWaHTIfXt+7GnZCIo=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"jadequer.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.22.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"show_result":true,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":false,"async":false,"duration":200,"transition":{"menu_item":false,"post_block":false,"post_header":false,"post_body":false,"coll_header":false,"sidebar":false}},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="ç›´æ¥pip install å®‰è£…flash-attnä¼šé‡è§å„ç§æŠ¥é”™ï¼Œå°è¯•äº†å¾ˆå¤šæ–¹æ³•ï¼š 1ã€é€šè¿‡ä» GitHub Releases é¡µé¢ä¸‹è½½æ­£ç¡®çš„è½®æ–‡ä»¶ https:&#x2F;&#x2F;github.com&#x2F;Dao-AILab&#x2F;flash-attention&#x2F;releases é€‰æ‹©åˆé€‚ç‰ˆæœ¬ï¼Œå¦‚æœ¬é¡¹ç›®è¦æ±‚çš„python 3.10, cuda 12.4, torch 2.4å°†é€‰æ‹©ä»¥ä¸‹ç‰ˆæœ¬ https:&#x2F;&#x2F;gi">
<meta property="og:type" content="website">
<meta property="og:title" content="å®‰è£…flash-attn">
<meta property="og:url" content="https://jadequer.github.io/blog/records/flash-attn.html">
<meta property="og:site_name" content="Arctic&#39;s Blog">
<meta property="og:description" content="ç›´æ¥pip install å®‰è£…flash-attnä¼šé‡è§å„ç§æŠ¥é”™ï¼Œå°è¯•äº†å¾ˆå¤šæ–¹æ³•ï¼š 1ã€é€šè¿‡ä» GitHub Releases é¡µé¢ä¸‹è½½æ­£ç¡®çš„è½®æ–‡ä»¶ https:&#x2F;&#x2F;github.com&#x2F;Dao-AILab&#x2F;flash-attention&#x2F;releases é€‰æ‹©åˆé€‚ç‰ˆæœ¬ï¼Œå¦‚æœ¬é¡¹ç›®è¦æ±‚çš„python 3.10, cuda 12.4, torch 2.4å°†é€‰æ‹©ä»¥ä¸‹ç‰ˆæœ¬ https:&#x2F;&#x2F;gi">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://jadequer.github.io/blog/records/flash-attn.assets/image-20251027200248889.png">
<meta property="article:published_time" content="2025-10-29T02:02:04.000Z">
<meta property="article:modified_time" content="2025-10-29T04:32:06.612Z">
<meta property="article:author" content="Arctic">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://jadequer.github.io/blog/records/flash-attn.assets/image-20251027200248889.png">


<link rel="canonical" href="https://jadequer.github.io/blog/records/flash-attn">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":false,"lang":"en","comments":true,"permalink":"https://jadequer.github.io/blog/records/flash-attn.html","path":"blog/records/flash-attn.html","title":"å®‰è£…flash-attn"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>å®‰è£…flash-attn | Arctic's Blog
</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Arctic's Blog</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">ä¸€ä¸ªCSå­¦ç”Ÿçš„ä¸ªäººåšå®¢</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/me" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">2</span></a></li><li class="menu-item menu-item-blog"><a href="/blog/" rel="section"><i class="fa fa-list fa-fw"></i>blog</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Arctic</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">2</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner page posts-expand">


    
    
    
    <div class="post-block" lang="en"><header class="post-header">

<h1 class="post-title" itemprop="name headline">å®‰è£…flash-attn
</h1>

<div class="post-meta-container">
  <ul class="breadcrumb">
            <li><a href="/blog/">BLOG</a></li>
            <li><a href="/blog/records/">RECORDS</a></li>
            <li>FLASH-ATTN</li>
  </ul>
</div>

</header>

      
      
      
      <div class="post-body">
          <p>ç›´æ¥pip install å®‰è£…flash-attnä¼šé‡è§å„ç§æŠ¥é”™ï¼Œå°è¯•äº†å¾ˆå¤šæ–¹æ³•ï¼š</p>
<p>1ã€é€šè¿‡ä» GitHub Releases é¡µé¢ä¸‹è½½æ­£ç¡®çš„è½®æ–‡ä»¶</p>
<p>https://github.com/Dao-AILab/flash-attention/releases</p>
<p>é€‰æ‹©åˆé€‚ç‰ˆæœ¬ï¼Œå¦‚æœ¬é¡¹ç›®è¦æ±‚çš„python 3.10, cuda 12.4, torch
2.4å°†é€‰æ‹©ä»¥ä¸‹ç‰ˆæœ¬</p>
<p>https://github.com/Dao-AILab/flash-attention/releases/download/v2.8.3/flash_attn-2.8.3+cu12torch2.4cxx11abiTRUE-cp310-cp310-linux_x86_64.whl</p>
<p>å‘½ä»¤ä¸ºï¼š<code>pip install https://github.com/Dao-AILab/flash-attention/releases/download/v2.8.3/flash_attn-2.8.3+cu12torch2.4cxx11abiTRUE-cp310-cp310-linux_x86_64.whl</code></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install pytorch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 pytorch-cuda=12.4 -c pytorch -c nvidia</span><br></pre></td></tr></table></figure>
<p>è¿è¡Œbash.shåæŠ¥å¦‚ä¸‹é”™ï¼š</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">(magi) root@7s6go2hlu6o82-0:/duwenqing/JIANGQINRU/MAGI-1# bash example/4.5B/run.sh</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">&quot;/duwenqing/JIANGQINRU/MAGI-1/inference/pipeline/entry.py&quot;</span>, line 18, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">    from inference.pipeline import MagiPipeline</span><br><span class="line">  File <span class="string">&quot;/duwenqing/JIANGQINRU/MAGI-1/inference/pipeline/__init__.py&quot;</span>, line 15, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">    from .pipeline import MagiPipeline</span><br><span class="line">  File <span class="string">&quot;/duwenqing/JIANGQINRU/MAGI-1/inference/pipeline/pipeline.py&quot;</span>, line 20, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">    from inference.model.dit import get_dit</span><br><span class="line">  File <span class="string">&quot;/duwenqing/JIANGQINRU/MAGI-1/inference/model/dit/__init__.py&quot;</span>, line 15, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">    from .dit_model import get_dit</span><br><span class="line">  File <span class="string">&quot;/duwenqing/JIANGQINRU/MAGI-1/inference/model/dit/dit_model.py&quot;</span>, line 39, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">    from .dit_module import CaptionEmbedder, FinalLinear, LearnableRotaryEmbeddingCat, TimestepEmbedder, TransformerBlock</span><br><span class="line">  File <span class="string">&quot;/duwenqing/JIANGQINRU/MAGI-1/inference/model/dit/dit_module.py&quot;</span>, line 27, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">    from flash_attn import flash_attn_varlen_func</span><br><span class="line">  File <span class="string">&quot;/duwenqing/miniconda3/envs/magi/lib/python3.10/site-packages/flash_attn/__init__.py&quot;</span>, line 3, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">    from flash_attn.flash_attn_interface import (</span><br><span class="line">  File <span class="string">&quot;/duwenqing/miniconda3/envs/magi/lib/python3.10/site-packages/flash_attn/flash_attn_interface.py&quot;</span>, line 15, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">    import flash_attn_2_cuda as flash_attn_gpu</span><br><span class="line">ImportError: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.32<span class="string">&#x27; not found (required by /duwenqing/miniconda3/envs/magi/lib/python3.10/site-packages/flash_attn_2_cuda.cpython-310-x86_64-linux-gnu.so)</span></span><br></pre></td></tr></table></figure>
<p>ç”±äºç³»ç»Ÿæ˜¯ubuntu20.04ï¼Œå®‰è£…ä¸Šé¢ç‰ˆæœ¬ä¾æ—§æ— æ³•è¿è¡Œï¼Œglibcï¼ˆGNU
Cåº“ï¼‰ç‰ˆæœ¬è¿‡ä½ï¼Œè€Œ <code>flash-attn</code>
çš„äºŒè¿›åˆ¶æ‰©å±•æ¨¡å—ï¼ˆ<code>.so</code> æ–‡ä»¶ï¼‰æ˜¯åœ¨ä¸€ä¸ª <strong>glibc â‰¥
2.32</strong> çš„ç³»ç»Ÿä¸Šç¼–è¯‘çš„</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ldd --version</span><br></pre></td></tr></table></figure>
<p>è¾“å‡ºï¼š</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ldd (Ubuntu GLIBC 2.31-0ubuntu9.14) 2.31</span><br></pre></td></tr></table></figure>
<p>ç”±äºç›´æ¥æ ¹æ®python 3.10, cuda 12.4, torch
2.4åŒ¹é…åˆ°çš„æ˜¯2.8.3ç‰ˆæœ¬ï¼Œè€Œrequirement.txtä¸­æ˜¯2.4.2ï¼Œå¯èƒ½å®‰è£…2.4.2èƒ½è§£å†³</p>
<p>2ã€å®‰è£…requirement.txtä¸­çš„2.4.2ç‰ˆæœ¬ï¼ˆå¯èƒ½æœ‰ç”¨ï¼Œä¹Ÿå¯èƒ½æ²¡ç”¨ï¼‰ï¼Œæ ¹æ®chatgptæç¤ºï¼Œè£…é¢„ç¼–è¯‘
wheelï¼š</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">(magi) root@7s6go2hlu6o82-0:/duwenqing# pip install flash-attn==2.4.2 --index-url https://flash-attention-wheels.s3.us-west-2.amazonaws.com/whl/cu124/torch2.4/index.html</span><br><span class="line">Looking <span class="keyword">in</span> indexes: https://flash-attention-wheels.s3.us-west-2.amazonaws.com/whl/cu124/torch2.4/index.html, https://pypi.ngc.nvidia.com</span><br><span class="line"></span><br><span class="line">ERROR: Could not find a version that satisfies the requirement flash-attn==2.4.2 (from versions: none)</span><br><span class="line">ERROR: No matching distribution found <span class="keyword">for</span> flash-attn==2.4.2</span><br><span class="line">(magi) root@7s6go2hlu6o82-0:/duwenqing# nvcc -V</span><br><span class="line">nvcc: NVIDIA (R) Cuda compiler driver</span><br><span class="line">Copyright (c) 2005-2025 NVIDIA Corporation</span><br><span class="line">Built on Wed_Apr__9_19:24:57_PDT_2025</span><br><span class="line">Cuda compilation tools, release 12.9, V12.9.41</span><br><span class="line">Build cuda_12.9.r12.9/compiler.35813241_0</span><br></pre></td></tr></table></figure>
<p>è¿™ä¸ªè¾“å‡ºè¡¨æ˜ï¼Œç°åœ¨çš„ ç³»ç»Ÿ CUDA ç¼–è¯‘å™¨ï¼ˆ12.9ï¼‰æ¯” PyTorch çš„ runtime
CUDAï¼ˆ12.4ï¼‰é«˜ä¸€ä¸ªå¤§ç‰ˆæœ¬ã€‚è¿™ä¼šå¯¼è‡´ flash-attn çš„ wheel
æºï¼ˆcu124/torch2.4ï¼‰æ‰¾ä¸åˆ°åŒ¹é…é¡¹ï¼Œå› ä¸ºå®˜æ–¹è¿˜æ²¡æä¾›é’ˆå¯¹ CUDA 12.9 + Torch
2.4 çš„é¢„ç¼–è¯‘åŒ…ã€‚</p>
<p>æ‰€ä»¥ï¼Œä¿æŒ Torch 2.4ï¼Œä¸åŠ¨ï¼Œé™çº§ CUDA toolkitï¼š</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install -c nvidia cuda-toolkit=12.1</span><br></pre></td></tr></table></figure>
<p>è¿˜æ˜¯ä¸è¡Œ</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">(magi) root@7s6go2hlu6o82-0:/duwenqing# pip install flash-attn==2.4.2 --index-url https://flash-attention-wheels.s3.us-west-2.amazonaws.com/whl/cu121/torch2.4/index.html</span><br><span class="line"></span><br><span class="line">Looking <span class="keyword">in</span> indexes: https://flash-attention-wheels.s3.us-west-2.amazonaws.com/whl/cu121/torch2.4/index.html, https://pypi.ngc.nvidia.com</span><br><span class="line">ERROR: Could not find a version that satisfies the requirement flash-attn==2.4.2 (from versions: none)</span><br><span class="line">ERROR: No matching distribution found <span class="keyword">for</span> flash-attn==2.4.2</span><br></pre></td></tr></table></figure>
<p>ä¸‹é¢æ¥è‡ªGitHub Releases é¡µé¢ï¼š</p>
<figure>
<img src="flash-attn.assets/image-20251027200248889.png"
alt="image-20251027200248889" />
<figcaption aria-hidden="true">image-20251027200248889</figcaption>
</figure>
<p>å¦‚æœè¦è£…2.4.2ï¼Œé‚£ä¹ˆtorchå’Œcudaç‰ˆæœ¬éƒ½ä¸èƒ½æŒ‰ç…§READMEä¸­æ¥ï¼Œå¯èƒ½æœ€åé¡¹ç›®ä¹Ÿè¿è¡Œä¸èµ·æ¥</p>
<p>ä»¥ä¸‹æ˜¯MAGI-1çš„READMEï¼š</p>
<p><strong>Run with Docker Environment (Recommend)</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker pull sandai/magi:latest</span><br><span class="line"></span><br><span class="line">docker run -it --gpus all --privileged --shm-size=32g --name magi --net=host --ipc=host --<span class="built_in">ulimit</span> memlock=-1 --<span class="built_in">ulimit</span> stack=6710886 sandai/magi:latest /bin/bash</span><br></pre></td></tr></table></figure>
<p><strong>Run with Source Code</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a new environment</span></span><br><span class="line">conda create -n magi python==3.10.12</span><br><span class="line"></span><br><span class="line"><span class="comment"># Install pytorch</span></span><br><span class="line">conda install pytorch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 pytorch-cuda=12.4 -c pytorch -c nvidia</span><br><span class="line"></span><br><span class="line"><span class="comment"># Install other dependencies</span></span><br><span class="line">pip install -r requirements.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Install ffmpeg</span></span><br><span class="line">conda install -c conda-forge ffmpeg=4.4</span><br><span class="line"></span><br><span class="line"><span class="comment"># For GPUs based on the Hopper architecture (e.g., H100/H800), it is recommended to install MagiAttention(https://github.com/SandAI-org/MagiAttention) for acceleration. For non-Hopper GPUs, installing MagiAttention is not necessary.</span></span><br><span class="line">git <span class="built_in">clone</span> git@github.com:SandAI-org/MagiAttention.git</span><br><span class="line"><span class="built_in">cd</span> MagiAttention</span><br><span class="line">git submodule update --init --recursive</span><br><span class="line">pip install --no-build-isolation .</span><br></pre></td></tr></table></figure>
<p>æœ€åæœ‰ä¸€ä¸ªä»æºç ç¼–è¯‘ flash-attnçš„åŠæ³•ï¼Œä¼šåœ¨å½“å‰ç³»ç»Ÿç¯å¢ƒä¸‹é“¾æ¥æœ¬åœ°
glibcã€‚</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip uninstall -y flash-attn</span><br><span class="line">pip install flash-attn==2.4.2 --no-build-isolation --no-cache-dir</span><br></pre></td></tr></table></figure>
<p>ä½†æ˜¯ä¸€ç›´å¡ä½,ï¼Œå¯èƒ½æ˜¯æ˜¾å¡å’Œ CPUä¸å¤Ÿï¼š</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">DEPRECATION: Building <span class="string">&#x27;flash-attn&#x27;</span> using the legacy setup.py bdist_wheel mechanism, <span class="built_in">which</span> will be removed <span class="keyword">in</span> a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the --use-pep517 option, (possibly combined with --no-build-isolation), or adding a pyproject.toml file to the <span class="built_in">source</span> tree of <span class="string">&#x27;flash-attn&#x27;</span>. Discussion can be found at https://github.com/pypa/pip/issues/6334 </span><br><span class="line">Building wheel <span class="keyword">for</span> flash-attn (setup.py) ... /</span><br></pre></td></tr></table></figure>
<p>æ‰§è¡Œfree -h æŸ¥çœ‹å†…å­˜å’Œswapç©ºé—´ï¼š</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(magi_try) root@7s6go2hlu6o82-0:/duwenqing/JIANGQINRU/MAGI-1# free -h</span><br><span class="line">              total        used        free      shared  buff/cache   available</span><br><span class="line">Mem:          503Gi       146Gi       337Gi       4.4Gi        18Gi       351Gi</span><br><span class="line">Swap:            0B          0B          0B</span><br></pre></td></tr></table></figure>
<p>å‘ç°æ˜¯æ²¡æœ‰swapç©ºé—´</p>
<p>å°è¯•ä»¥ä¸‹åŠæ³•ï¼š</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># åˆ›å»ºä¸€ä¸ª 8GB çš„äº¤æ¢æ–‡ä»¶</span></span><br><span class="line"><span class="built_in">sudo</span> fallocate -l 8G /swapfile</span><br><span class="line"><span class="comment"># è®¾ç½®æƒé™</span></span><br><span class="line"><span class="built_in">sudo</span> <span class="built_in">chmod</span> 600 /swapfile</span><br><span class="line"><span class="comment"># æ ¼å¼åŒ–ä¸º swap ç©ºé—´</span></span><br><span class="line"><span class="built_in">sudo</span> mkswap /swapfile</span><br><span class="line"><span class="comment"># å¯ç”¨ swap</span></span><br><span class="line"><span class="built_in">sudo</span> swapon /swapfile</span><br></pre></td></tr></table></figure>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> fallocate -l 8G /mnt/swapfile</span><br><span class="line"><span class="built_in">sudo</span> <span class="built_in">chmod</span> 600 /mnt/swapfile</span><br><span class="line"><span class="built_in">sudo</span> mkswap /mnt/swapfile</span><br><span class="line"><span class="built_in">sudo</span> swapon /mnt/swapfile</span><br></pre></td></tr></table></figure>
<p>å‡æ— æœã€‚</p>
<p>å¦‚æœèƒ½ä½¿ç”¨dockerç¯å¢ƒåº”è¯¥æ˜¯æœ€æ–¹ä¾¿çš„ï¼Œå¦‚æœä¸è¡Œï¼Œåº”è¯¥éœ€è¦æ›´å¥½çš„æœåŠ¡å™¨é…ç½®æ‰èƒ½é…ä¸Šè¿™ä¸ªç¯å¢ƒâ€¦â€¦</p>
<p>è¿™è¡Œï¼š</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Building wheel for flash-attn (setup.py) ... \</span><br></pre></td></tr></table></figure>
<p>å¹¶ä¸ä¸€å®šæ˜¯â€œå¡æ­»â€ï¼Œæœ‰æ—¶æ˜¯ <strong>æ­£åœ¨ç¼–è¯‘ CUDA æ ¸å¿ƒä»£ç ï¼ˆéå¸¸è€—å†…å­˜ +
CPU + GPUï¼‰</strong>ï¼Œä½†ä¹Ÿå¯èƒ½æ˜¯çœŸçš„æŒ‚æ‰äº†ã€‚</p>
<p>è¿è¡Œä»¥ä¸‹å‘½ä»¤çœ‹çœ‹æ˜¯å¦è¿˜åœ¨ç”¨ CPUï¼š</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">top -p $(pgrep -d, -f flash)</span><br></pre></td></tr></table></figure>
<p>æˆ–è€…æ›´ä¿é™©ä¸€ç‚¹ï¼š</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ps aux | grep &quot;gcc\|nvcc\|ninja&quot;</span><br></pre></td></tr></table></figure>
<p><strong>å¦‚æœçœ‹åˆ°æœ‰ gcc / nvcc / ninja åœ¨è·‘ã€CPU å ç”¨ &gt;
50%ï¼Œè¯´æ˜è¿˜åœ¨ç¼–è¯‘ã€‚</strong> FlashAttention ä¼šç¼–è¯‘åå‡ ä¸ª CUDA
kernelï¼Œæ¯ä¸ª kernel ç¼–è¯‘å¯èƒ½è¦å‡ åˆ†é’Ÿã€‚</p>
<blockquote>
<p>âš ï¸ åœ¨ 8GB å†…å­˜ + 0 swap çš„æƒ…å†µä¸‹ï¼Œç¼–è¯‘ 2.8.3 å¾ˆå®¹æ˜“è¢« kill
æˆ–å¡æ­»ã€‚</p>
</blockquote>
<p>å¦‚æœå†…å­˜æˆ– swap ä¸è¶³ï¼Œå‡ ä¹å¿…æŒ‚</p>
<p>ç³»ç»Ÿå½“å‰çŠ¶æ€ï¼š</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">MiB Mem : 1031704 total, 130073 free, 29171 used, 872458 buff/cache</span><br><span class="line">MiB Swap: 0 total</span><br></pre></td></tr></table></figure>
<p>ğŸ‘‰ <strong>Swap = 0</strong>ï¼Œè¿™å°±æ„å‘³ç€ä¸€æ—¦å†…å­˜è¢«åƒæ»¡ï¼ˆç¼–è¯‘
flash-attn å¤§çº¦éœ€è¦ 6ï½8GBï¼‰ï¼Œ ç³»ç»Ÿä¼šç›´æ¥ <strong>åœä½æˆ–è¢« OOM
Kill</strong>ã€‚</p>
<p>æ­¤æ—¶â€œå¡ä½â€å…¶å®æ˜¯å› ä¸ºç¼–è¯‘å™¨è¿›ç¨‹è¢«æŒ‚èµ·æˆ–ä¸­æ­¢ã€‚</p>
<p>æœ€åæœ€åçš„è§£å†³åŠæ³•ï¼Œå…¶å®å°±æ˜¯ç¬¬ä¸€ä¸ªï¼Œ åœ¨GitHub Releases
é¡µé¢ä¸‹è½½é¢„ç¼–è¯‘æ–‡ä»¶ï¼š</p>
<p>ç¬¬ä¸€æ¬¡ä¸æˆæ˜¯å› ä¸ºç³»ç»Ÿç‰ˆæœ¬ä¸å¤Ÿã€‚æ¢æˆ22.04å°±å¯ä»¥äº†ã€‚</p>
<p>ä½†æ˜¯å½“æ—¶ç‰ˆæœ¬é€‰é”™äº†ï¼Œè¿˜éœ€è¦æŸ¥çœ‹pythonçš„ABIçŠ¶æ€ï¼š</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -c <span class="string">&quot;import torch; print(torch.compiled_with_cxx11_abi())&quot;</span></span><br></pre></td></tr></table></figure>
<p>é€‰æ‹©æ­£ç¡®åŒ¹é…çš„ç‰ˆæœ¬å³å¯</p>

      </div>
      
      
      
    </div>
  <ul class="breadcrumb">
            <li><a href="/blog/">BLOG</a></li>
            <li><a href="/blog/records/">RECORDS</a></li>
            <li>FLASH-ATTN</li>
  </ul>

    
    


</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Arctic</span>
  </div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/JadeQuer" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  

  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/11.4.0/mermaid.min.js","integrity":"sha256-G8ouPAnw4zzMbnAenHnVz6h9XpKbNdOkrqTh7AadyHs="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>





  




  

  <script class="next-config" data-name="enableMath" type="application/json">false</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
